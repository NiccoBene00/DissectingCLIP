# -*- coding: utf-8 -*-
"""Class-Agnostic_HighRes_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kkgXSnzYFSoXxL0Hp2T2fH_JXneNhHCW

##Class-agnostic with Higher Resolution Dataset

We're about to experiment class-agnostic setting with Tiny ImageNet (64x64 image
resolution and 200 generic different classes) and Food101 (512x512 image resolution and 101 food-based classes).
Extract text input information switching ```promptsCIFAR``` with ```promptsFood101``` in order to obtain results described within the thesis.
"""

promptsCIFAR = [
    "a photo of an object",
    "a photo of something natural",
    "a photo of something man-made",
    "a blurry photo",
    "a close-up photo",
    "a photo taken during the day",
    "a photo taken outdoors",
    "a photo taken indoors",
    "a small object",
    "a large object",
    "an animal",
    "a machine",
    "a thing that moves",
    "a stationary object",
    "a photo of a living thing",
    "a photo of a synthetic thing",
    "a colorful object",
    "a grayscale image",
    "a smooth surface",
    "a textured object",
    "a plastic object",
    "a metallic object",
    "a flying thing",
    "a photo of something round",
    "a rectangular object",
    "a noisy image",
    "an object with wheels",
    "an object with wings",
    "an underwater object",
    "a photo from a low angle",
    "a photo from a top view",
    "a centered object",
]
promptsFood101 = [
    "a photo of a sweet dish",
    "a photo of a spicy meal",
    "a photo of a baked dessert",
    "a photo of a traditional snack",
    "a photo of a grilled meat",
    "a photo of an Asian dish",
    "a photo of a fried appetizer",
    "a photo of an Italian recipe",
    "a photo of a colorful salad",
    "a photo of a meat-based dish",
    "a photo of a creamy dessert",
    "a photo of a cold drink",
    "a photo of a cheesy food",
    "a photo of a rice-based meal",
    "a photo of a noodle dish",
    "a photo of a breakfast item",
    "a photo of a fast food meal",
    "a photo of a sweet pastry",
    "a photo of a chocolate dessert",
    "a photo of a dairy-based dish",
    "a photo of a stuffed vegetable",
    "a photo of a hot soup",
    "a photo of a crispy dish",
    "a photo of a seafood plate",
    "a photo of a fruit dessert",
    "a photo of a sweetened bread",
    "a photo of a finger food",
    "a photo of a traditional dessert",
    "a photo of a comfort food",
    "a photo of a street food item",
    "a photo of a savory tart",
    "a photo of a festive meal"
]

!pip install -q transformers datasets torchvision matplotlib scikit-learn

"""##Tiny ImageNet dataset

Dataset preparation: downloading and extracting 100 random class of the 200 available within Tiny ImageNet.
"""

import shutil
from PIL import Image
import zipfile
import os
import urllib.request
import random
import torch
from torchvision import transforms
from torchvision.datasets import ImageFolder

data_dir = "./data/tiny-imagenet-100"
tiny_imagenet_url = "http://cs231n.stanford.edu/tiny-imagenet-200.zip"

if not os.path.exists(data_dir):
    print("Downloading Tiny ImageNet...")
    urllib.request.urlretrieve(tiny_imagenet_url, "./tiny-imagenet-200.zip")
    with zipfile.ZipFile("./tiny-imagenet-200.zip", "r") as zip_ref:
        zip_ref.extractall("./data")
    print("Extraction completed.")

# select 100 random class from the original 200
full_data_path = "./data/tiny-imagenet-200"
all_classes = os.listdir(os.path.join(full_data_path, "train"))
selected_classes = sorted(random.sample(all_classes, 100))

# copy only the selected 100 classes into the folder
os.makedirs(f"{data_dir}/train", exist_ok=True)
os.makedirs(f"{data_dir}/val/images", exist_ok=True)

print("Creating dataset with 100 classes...")
for cls in selected_classes:
    src = os.path.join(full_data_path, "train", cls)
    dst = os.path.join(data_dir, "train", cls)
    shutil.copytree(src, dst, dirs_exist_ok=True)


val_annotations = os.path.join(full_data_path, "val", "val_annotations.txt")
new_val_dir = os.path.join(data_dir, "val", "images")
os.makedirs(new_val_dir, exist_ok=True)

# Read and filter images that belong to the selected classes
with open(val_annotations, "r") as f:
    for line in f:
        tokens = line.strip().split('\t')
        img_name, cls = tokens[0], tokens[1]
        if cls in selected_classes:
            src_img = os.path.join(full_data_path, "val", "images", img_name)
            cls_dir = os.path.join(new_val_dir, cls)
            os.makedirs(cls_dir, exist_ok=True)
            shutil.copy(src_img, os.path.join(cls_dir, img_name))

print("Dataset Tiny ImageNet-100 ready!")

transform = transforms.Compose([
    transforms.Resize((224, 224)),
])

train_set_TynIm100 = ImageFolder(root=f"{data_dir}/train", transform=transform)
test_set_TynIm100 = ImageFolder(root=f"{data_dir}/val/images", transform=transform)

print(f"Train set size Tiny ImageNet: {len(train_set_TynIm100)}")
print(f"Test set size Tiny ImageNet: {len(test_set_TynIm100)}")

"""##Food101 dataset

Dataset preparation directly form torch.vision.
"""

from torchvision.datasets import Food101
from torchvision import transforms

transform = transforms.Compose([
    transforms.Resize((224, 224)),

])

train_set_Fo101 = Food101(root="./data", split="train", download=True, transform=transform)
test_set_Fo101 = Food101(root="./data", split="test", download=True, transform=transform)

print(f"Train set size Food101: {len(train_set_Fo101)}")
print(f"Test set size Food101: {len(test_set_Fo101)}")

import torch
from transformers import CLIPProcessor, CLIPModel

device = "cuda" if torch.cuda.is_available() else "cpu"
print("device", device)

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# Extracting Input Information

# Tokenize and encode the prompts
text_inputs = processor(promptsCIFAR, return_tensors = "pt", padding = True,
                             truncation = True).to(device)

with torch.no_grad():
  text_features = model.get_text_features(**text_inputs)

# Normalize text features
text_features = text_features / text_features.norm(dim = 1, keepdim = True)

print(f"Encoded {len(promptsFood101)} prompts into embeddings of shape: {text_features.shape}")

import torch.nn.functional as F
import numpy as np
from tqdm import tqdm
from torch.utils.data import DataLoader

# DataLoader for batching
def collate_fn(batch):
  images = [item[0] for item in batch]
  labels = [item[1] for item in batch]
  inputs = processor(images = images, return_tensors = "pt", padding = True).to(device)
  return inputs, torch.tensor(labels, dtype = torch.long)

train_loader = DataLoader(train_set_TynIm100, batch_size = 64, shuffle = False, collate_fn = collate_fn)
test_loader = DataLoader(test_set_TynIm100, batch_size = 64, shuffle = False, collate_fn = collate_fn)

# Generate class-agnostic logit features
def extract_logit_features(data_loader, text_features):
  image_features_list = []
  labels_list = []

  entropies = []

  for inputs, labels in tqdm(data_loader):
    with torch.no_grad():
      image_features = model.get_image_features(**inputs)
      image_features = image_features / image_features.norm(dim = 1, keepdim = True)

      logits = (image_features @ text_features.T) # cosine similarity
      # these similarity scores become the features for our downstream SVM

      # Entropy Analysis
      probs = F.softmax(logits, dim = 1) # softmax transforms logit into
                                         # normalized probabilities

      # so probs is a tensor of shape [batch_size, num_classes] where the sum
      # along dim = 1 is always one

      # sparsity metrics
      entropy = -torch.sum(probs * torch.log(probs + 1e-9), dim = 1)
      # we have to add + 1e-9 since log(0) would give -inf (numerical unstable)
      # entropy is a vector of batch_size values: i.e. entropy of each logit

      entropies.append(entropy.cpu().numpy())

    image_features_list.append(logits.cpu().numpy())
    labels_list.append(labels.cpu().numpy())

  features = np.concatenate(image_features_list, axis = 0)
  labels = np.concatenate(labels_list, axis = 0)

  all_entropy = np.concatenate(entropies)

  print(f"\nSparsity analysis through mean entropy: {all_entropy.mean():.4f}")

  return features, labels


# Run on train and test sets
train_features, train_labels = extract_logit_features(train_loader, text_features)
test_features, test_labels = extract_logit_features(test_loader, text_features)

print(f"Train features shape: {train_features.shape}")  # e.g., (#img train dataset, #prompt)
print(f"Test features shape: {test_features.shape}")    # e.g., (#img test dataset, #prompt)

!pip install scikit-learn -q

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Train a simple linear SVM classifier
svm_clf = SVC(kernel='linear', C=1.0, random_state=42)
svm_clf.fit(train_features, train_labels)

# Predict on the test set
test_preds = svm_clf.predict(test_features)

# Evaluate
test_acc = accuracy_score(test_labels, test_preds)
print(f"\nTest accuracy using Linear SVM: {test_acc * 100:.2f}%")



"""-----------------------------------------"""