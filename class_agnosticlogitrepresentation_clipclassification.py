# -*- coding: utf-8 -*-
"""Class-AgnosticLogitRepresentation_CLIPClassification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jixmPtM34spK2TVaXoyZ0tpt0IKvg5It

#Class-Agnostic Logit Representation for CLIP classification

Class-agnostic means a model or representation does not depend on knowing or using specific class labels during training. Instead of building representations specifically aligned with known label (e.g. *airplane*, *cat*, *dog*, etc), we aim to learn features that are generic and task-indipendence.
<br>
<br>
In the contex of CLIP or other large pretrained models, a logit is the output of the model itself before applying ```softmax```- i.e. the raw similarity scores (for zero-shot) or linear classifier outputs for **SVM**. While a class-agnostic logit representation refers to using logit outputs without binding them to specific class labels at extraction time. In other words we have to treat the logits as feature vectors that encode the model's belief or alignment across a space of concepts.
<br>
<br>
The idea here is trying to extract the raw similarity logits between an image and a large number of generic prompts or learned queries, hence training a classifier (like **SVM**) on top of those raw logit features, rather than image embeddings as done for the baseline CLIP classification.
<br>
<br>
With all that just being said, we can see this case-of-study as an attempt to respond at the following questions:
* Are the image/text similarity logits sematically rich enough to classify objects even without knowing class labels during features extraction?
* Can CLIP's internal similarity structure generalize to new tasks or datasets without tuning?
* What part of CLIP contains the most *useful* information - embeddings or logits?

## Linear Probe Paradigm (LPP)

What we are about to do is well known as **probe-based supervise evaluation of unsupervised or generic representation**. Substantially we are using CLIP's pretrained representation to evaluate how well it encodes class-discriminative information, without giving it direct supervision during representation extraction. Then we're going to train **SVM** on those features to evaluate their quality. So, in other words, we're not training CLIP, but we're testing whether CLIP's embeddings already contain enough info to separate different class. In this scenario **SVM** acts as a simple "probe" to measure quality.


-----------------------------------------

##Step1 LPP: Define a Class-Agnostic Prompt Set

We'd like to create a list of generic, class-agnostic prompts that capture broad, semantic attributes or contexts, rather than actual CIFAR-100 class names (e.g. no *airplane*, *frog*, etc). These prompts will be passed through the CLIP text encoder, and we'll extract the image-to-prompt similarity logits to form a new feature vector for each image.
"""

# Define class-agnostic prompt set
# The idea is to explore general visual concepts that might describe
# features CLIP has learned. These are class-agnostic descriptions — not
# specific to CIFAR-100 labels like "apple" or "train".
# We’ll use these to probe how much generic visual understanding CLIP has.
prompts = [
    "a photo of an object",
    "a photo of something natural",
    "a photo of something man-made",
    "a blurry photo",
    "a close-up photo",
    "a photo taken during the day",
    "a photo taken outdoors",
    "a photo taken indoors",
    "a small object",
    "a large object",
    "an animal",
    "a machine",
    "a thing that moves",
    "a stationary object",
    "a photo of a living thing",
    "a photo of a synthetic thing",
    "a colorful object",
    "a grayscale image",
    "a smooth surface",
    "a textured object",
    "a plastic object",
    "a metallic object",
    "a flying thing",
    "a photo of something round",
    "a rectangular object",
    "a noisy image",
    "an object with wheels",
    "an object with wings",
    "an underwater object",
    "a photo from a low angle",
    "a photo from a top view",
    "a centered object",
]

prompts16 = [
    "a photo of a  puppy",         
    "a photo of a red parrot ", 
    "a photo of a shark in the ocean",     
    "a photo of a bumblebee on a flower",           
    "a photo of a toddler",   
    "a photo of a yellow object",    
    "a photo of a vintage  car",         
    "a photo of a military jet",            
    "a photo of a shiny stainless steel pan",       
    "a photo of a white pillow",             
    "a photo of a watermelon on a table",    
    "a photo of a bookshelf with books",     
    "a photo taken in a sunny park",                
    "a photo taken in a  kitchen",         
    "a photo of a house cat on a couch",   
    "a photo of a deer standing in the woods"       
]

"""## Step2 LPP: Encode Class-Agnostic Prompts with CLIP


We are about to use the pretrained ```openai/clip-vit-base-patch32``` model to encode our previously defined prompts into text embeddings. These embeddings will be compared with image embeddings to create class-agnostic logit-based feature vectors in the next step.

"""

!pip install -q transformers datasets torchvision matplotlib scikit-learn

import torch
from transformers import CLIPProcessor, CLIPModel

device = "cuda" if torch.cuda.is_available() else "cpu"
print("device", device)

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# Tokenize and encode the prompts
text_inputs = processor(prompts, return_tensors = "pt", padding = True,
                             truncation = True).to(device)

with torch.no_grad():
  text_features = model.get_text_features(**text_inputs)

# Normalize text features
text_features = text_features / text_features.norm(dim = 1, keepdim = True)

print(f"Encoded {len(prompts)} prompts into embeddings of shape: {text_features.shape}") # torch.Size [32, 512]

"""## Step3 LPP: Generate Class-Agnostic Logit Vectors

In the following section of code we'll extract the image embeddings using CLIP's vision encoder and we'll compute cosine similarity with each of the 32 class-agnostic prompt embeddings. Finally we'll store the resulting ```[1x32]``` logit vector as the final feature representation. These logit vectors are what we'll use to train a linear classifier in the next step.
"""

from torchvision.datasets import CIFAR100
from torch.utils.data import DataLoader
import numpy as np
from tqdm.notebook import tqdm
import torch.nn.functional as F

# Load CIFAR-100 dataset
train_dataset = CIFAR100(root="./data", download=True, train=True)
test_dataset = CIFAR100(root="./data", download=True, train=False)

# DataLoader for batching
def collate_fn(batch):
  images = [item[0] for item in batch]
  labels = [item[1] for item in batch]
  inputs = processor(images = images, return_tensors = "pt", padding = True).to(device)
  return inputs, torch.tensor(labels, dtype = torch.long)

train_loader = DataLoader(train_dataset, batch_size = 64, shuffle = False, collate_fn = collate_fn)
test_loader = DataLoader(test_dataset, batch_size = 64, shuffle = False, collate_fn = collate_fn)

# Generate class-agnostic logit features
def extract_logit_features(data_loader, text_features):
  image_features_list = []
  labels_list = []

  entropies = []

  for inputs, labels in tqdm(data_loader):
    with torch.no_grad():
      image_features = model.get_image_features(**inputs)
      image_features = image_features / image_features.norm(dim = 1, keepdim = True)

      logits = image_features @ text_features.T # cosine similarity
      # these similarity scores become the features for our downstream SVM

      # Entropy Analysis
      probs = F.softmax(logits, dim = 1) # softmax transforms logit into
                                         # normalized probabilities
      # so probs is a tensor of shape [batch_size, num_classes] where
      # the sum along dim = 1 is always one

      # Sparsity metrics
      entropy = -torch.sum(probs * torch.log(probs + 1e-9), dim = 1)
      # we have to add + 1e-9 since log(0) would give -inf (numerical unstable)
      # entropy is a vector of batch_size values: i.e. entropy of each logit
      
      entropies.append(entropy.cpu().numpy())
      
    image_features_list.append(logits.cpu().numpy())
    labels_list.append(labels.cpu().numpy())

  features = np.concatenate(image_features_list, axis = 0)
  labels = np.concatenate(labels_list, axis = 0)

  all_entropy = np.concatenate(entropies)
  print(f"Sparsity analysis through mean entropy: {all_entropy.mean():.4f}")   
    
  return features, labels


# Run on train and test sets
train_features, train_labels = extract_logit_features(train_loader, text_features)
test_features, test_labels = extract_logit_features(test_loader, text_features)

print(f"Train features shape: {train_features.shape}")  # e.g., (50000, 32)
print(f"Test features shape: {test_features.shape}")    # e.g., (10000, 32)

"""## Step4 LPP: Train a Classifier

Now we're ready to train a supervised linear classifier (**Logistic Regression** or **SVM**) using logit-based fetures extracted from CLIP in the previous step. This will help us measure how well generic semantic similarities can separate visual classes and whether CLIP's vision-language alignment generalizes without class-specific tuning.
"""

!pip install scikit-learn -q

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Train a simple linear SVM classifier
svm_clf = SVC(kernel='linear', C=1.0, random_state=42)
svm_clf.fit(train_features, train_labels)

# Predict on the test set
test_preds = svm_clf.predict(test_features)

# Evaluate
test_acc = accuracy_score(test_labels, test_preds)
print(f"\nTest accuracy using Linear SVM: {test_acc * 100:.2f}%")

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Train a simple Logistic Regression classifier
clf = LogisticRegression(max_iter = 1000) # max iterations ensures the solver
                                          # has enough iterations to converge
clf.fit(train_features, train_labels)

test_preds = clf.predict(test_features)
test_acc = accuracy_score(test_labels, test_preds)
print(f"\nTest accuracy using Logistic Regression: {test_acc *100:.2f}%")

cm = confusion_matrix(test_labels, test_preds)
class_names = test_dataset.classes
plt.figure(figsize=(20, 18))
sns.heatmap(cm, cmap="Blues", xticklabels=class_names, yticklabels=class_names)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.xticks(rotation=90, fontsize=6)
plt.yticks(rotation=0, fontsize=6)
plt.tight_layout()

plt.show()

print("Classification Report:")
print(classification_report(test_labels, test_preds, target_names=class_names))

"""## Final Insights

The results is just telling us we're getting 30x better than chance without any model retraining - which is really impressive. Indeed **30.33%** is far above random chance (1% for **CIFAR-100**). It shows that CLIP's features encode meaningful semantic information - even though they weren't trained specifically on **CIFAR-100**.

The aim of the next experiment will be understanding what problems we have to address in order to mitigate the limitations uncovered here.

-----------------------------------------
"""
