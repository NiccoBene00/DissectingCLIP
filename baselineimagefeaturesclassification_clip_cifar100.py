# -*- coding: utf-8 -*-
"""BaselineImageFeaturesClassification_CLIP_CIFAR100.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JUf-zPeraYqVfEL7ijIh4sr-cVsQ0UOe

# BASELINE IMAGE FEATURES CLASSIFICATION VIA CLIP

In this experiment, we're going to use CLIP (```openai/clip-vit-base-patch32```) as a frozen features extractor on the **CIFAR-100** image classification task. We will extract 512-dimensional image embeddings and we'll train a linear Support Vector Machine (**SVM**) as a classifier.

This experiment will serve as a baseline for common approaches to benchmark **zero-shot** or **few-shot** CLIP classification. Why is that?
Even though we're using an **SVM** (which does use training data), we are not training the CLIP model itself — we're just using its pre-trained knowledge to extract image features. So using CLIP's embeddings and training a simple model like **SVM**, we are benchmarking how goog CLIP's built-in features are and if the **SVM** does weel with little training data. It would mean CLIP already capture very meaning information - which is exactly what we'd expect from a **zero-shot/few-shot** capable model.

##Dependecies & Setup

Intalling dependencies and necessary libraries.
"""

!pip install -q transformers datasets torchvision matplotlib scikit-learn

# Import libraries
import torch
from torchvision.datasets import CIFAR100
from torchvision import transforms
from transformers import CLIPProcessor, CLIPModel
from torch.utils.data import DataLoader
from PIL import Image
import matplotlib.pyplot as plt

"""##Step1: Load CLIP model (```openai/clip-vit-base-patch32```) and CIFAR-100 dataset

We're about to load the CLIP model, its image processor (preprocessing) and **CIFAR-100** from Hugging Face.

*Some notes: CLIP expects images resized to 224x224, but CIFAR-10 images are 32x32. Resizing is mandatory.*
"""

model_name = "openai/clip-vit-base-patch32"
clip_model = CLIPModel.from_pretrained(model_name)

# Loads a processor that handles preprocessing for both image and text inputs,
# making them ready for the CLIP model
clip_processor = CLIPProcessor.from_pretrained(model_name)

# Put model on device
device = "cuda" if torch.cuda.is_available() else "cpu"
clip_model = clip_model.to(device).eval()

# Define a transform to convert CIFAR-100 images to PIL images. Indeed CLIP
# processor expects images as PIL Images, not as raw tensors.
to_pil = transforms.ToPILImage() # PIL := Python Imaging Library

# Load CIFAR-100 training dataset, which consists of 60,000 color images
# (32x32 pixels) in 100 classes (e.g., airplane, cat, truck).
train_dataset = CIFAR100(root="./data", train=True, download=True)
test_dataset = CIFAR100(root="./data", train=False, download=True)

print(f"To PIL image: {train_dataset[0][0]}") # PIL image details
                                              # it would be 32x32 with mode=RGB

"""##Step2: Extract Image Embeddings from CLIP

We're going to take each **CIFAR-100** image, preprocess it using  ```CLIPProcessor ```, pass it through the  ```CLIModel ``` to get image embeddings (features vectors). A the end we have to store these embeddings along with their labels for training our classifier.
"""

from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import numpy as np

# Custom Dataset to wrap CIFAR-100 and apply CLIP preprocessing
class CLIPImageDataset(Dataset):
    def __init__(self, cifar_dataset):
        self.dataset = cifar_dataset

    def __len__(self):
        return len(self.dataset) # It simply returns the number of samples in
                                 # the original CIFAR-100 dataset.


    def __getitem__(self, idx):
        image, label = self.dataset[idx] # Retrieves the idx-th (img, lbl) pair
                                         # from the original CIFAR-100 dataset
        # Process with CLIP processor
        processed = clip_processor(images=image, return_tensors="pt")

        # We return a tuple: (processed_image_tensor, label) — suitable for
        # training or inference
        # with squeeze we remove the extra batch dimension
        return processed["pixel_values"].squeeze(0), label

# Let's transform CIFAR-100 datasets into datasets compatible with the CLIP model
train_clip_dataset = CLIPImageDataset(train_dataset)
test_clip_dataset = CLIPImageDataset(test_dataset)

image_tensor, label = train_clip_dataset[0]

print("Image type:", type(image_tensor # torch.Tensor
print("Image shape:", image_tensor.shape) # [3, 224, 334]
print("Label type:", type(label)) # int
print("Label value:", label) # for example: 19

# dataloader allow us to iterate on batch data (generally 64 per time)
# shuffle = false means data are read in order
train_loader = DataLoader(train_clip_dataset, batch_size=64, shuffle=False)
test_loader = DataLoader(test_clip_dataset, batch_size=64, shuffle=False)

#Function for extracting embeddings
def extract_embeddings(dataloader):
    all_embeddings = []
    all_labels = []

    # we have to extract only img features embedding
              
    with torch.no_grad(): # I'm not training but just feature extraction
                          # (inference)
        for images, labels in tqdm(dataloader):
            images = images.to(device) # move img to GPU is it available

            # the next line returns a feature vector (embedding) for each image
            # — typically 512-dimensional for ViT-B/32
            embeddings = clip_model.get_image_features(images)
            embeddings = embeddings.cpu().numpy() # convert embeddings into
                                                  # NumPy arrar in order to use
                                                  # them on scikit-learn later
            all_embeddings.append(embeddings)
            all_labels.extend(labels.numpy()) # labels remains untouched 

    # np.vstack(...) stacks all embeddings vertically into a big matrix:
    # shape will be [N, D] where N = number of images, D = embedding size (512)
    # and convert the label list to NumPy array
    return np.vstack(all_embeddings), np.array(all_labels)

# Extract embeddings
train_features, train_labels = extract_embeddings(train_loader)
test_features, test_labels = extract_embeddings(test_loader)

# we just obtain such matrix where each row is the CLIP features (512-dimensional) of a single image 
print("\nTrain features shape:", train_features.shape) # 50000 x 512 = [N, D]
print("\nTest features shape:", test_features.shape) # 10000 x 512 = [N, D]

"""##Step3: Train a Classifier (SVM)

We're about to use  ```train_features``` and  ```trains_labels``` (CLIP image embeddings and **CIFAR-100** labels) from the previous step in order to train a classifier, and evaluate it.
We'll use Support Vector Machine (**SVM**) with a linear kernel from  ```scikit-learn``` and a standardization of features using  ```StandardScaler``` ('cause SVMs work better with scaled data).
"""

from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# Standardize features
# Standardization aims to achieve MEAN = 0 and STANDAR DEVIATION = 1 for each
# 512-dimension feature
scaler = StandardScaler() # Standardization ensures fair treatment of each
                          # dimension in the feature space

X_train_scaled = scaler.fit_transform(train_features)
# X_train_scaled is now a standardized version of the CLIP embeddings for the
# training set


X_test_scaled = scaler.transform(test_features) # no fit in order to avoid data
                                                # leakage, i.e. we don't show to the model
                                                # data it shouldn't know

# Train an SVM classifier
# C is a paramter for regulation strength
# kernel = 'linear' uses such linear decision boundary (since embeddings are
# already powerful)
# random_state = 42 for the reproducibility
svm_clf = SVC(kernel = 'linear', C = 1.0, random_state = 42)
svm_clf.fit(X_train_scaled, train_labels)

# Predict on test set
y_pred = svm_clf.predict(X_test_scaled) # Predicts the labels for the test set
                                        # using the trained SVM

# Evaluate
accuracy = accuracy_score(test_labels, y_pred)
print(f"Baseline CLIP image classification: {accuracy * 100:.2f}%")

"""##Step4: Analyze Result - Confusion Matrix & Graphs

We are about to visualize and interpret the classifier performance using:
  - **Confusion Matrix**: to see which classes are often confused;
  - **Classification Report**: to get precision, recall, F1 score;
"""

from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# CIFAR-100 class names
class_names = train_dataset.classes

cm = confusion_matrix(test_labels, y_pred)

# large figure enough to accomodate 100x100 matrix
plt.figure(figsize=(20, 18))

# Plot heatmap without annotations to avoid clutter
sns.heatmap(cm, cmap="Blues", xticklabels=class_names, yticklabels=class_names)

plt.title("Confusion Matrix", fontsize=16)
plt.xlabel("Predicted", fontsize=14)
plt.ylabel("True", fontsize=14)
plt.xticks(rotation=90, fontsize=6)
plt.yticks(rotation=0, fontsize=6)
plt.tight_layout()

plt.show()

# Print classification report
print("Classification Report:\n")
print(classification_report(test_labels, y_pred, target_names=class_names))

"""## Final Insights

**CIFAR-100** contains 100 fine-grained classes with greater visual similarity (e.g. different species or subtypes). The accuracy drop from 93.24% with **CIFAR-10** (see the experiment [here](https://colab.research.google.com/drive/1SehdC_hvCKnXFI4UtJyRhdws6h5POdSA?usp=drive_link)) to 78.01% and that reflects:

*   increased intra-class variability and inter-class similarity in **CIFAR-100**;
*   the limited resolution and complexity of CIFAR images (32x32), which can challenge even strong pretrained features.

The classification report high-performing classes (**F1 ≥ 0.90**) for classes like *pickup_truck*, *wardrobe*, *keyboard*, *chair*, etc; low-performing classes (**F1 ≤ 0.60**) for *beaver*, *shrew*, *seal*, *crocodile*, etc.
Human classes like *boy*, *girl*, *man*, *woman* perform moderately well (~0.75–0.82), maybe suggesting that CLIP captures some semantic consistency but struggles with nuanced identity-based features in low-res images.

As final consideration we observe that many animal classes underperform compared to objects or veichles. This indicates that CLIP (trained on large internet corpora) may prioritize iconic or culturally distinctive objects over fine-grained natural categories, especially when evaluated at low resolution (32x32).

-----------------------------------------
"""

"""
## Clustering

The following code is used to do clustering for the class-agnostic experiment as well, achieving results described in the thesis.

"""

from sklearn.cluster import KMeans
from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score
from collections import Counter

# data features normalizer
def scaled_softmax(features):
    # transform features vector into probabilty distribution (every row sum to 1)
    e_x = np.exp(features - np.max(features, axis=1, keepdims=True))
    return e_x / e_x.sum(axis=1, keepdims=True)

# Purity
def purity_score(y_true, y_pred):
    # matrix [key, value] where key is the ID of the cluster and value is the
    # list of the associated true lables to the cluster element
    # example: y_true = ["cat", "dog", "cat", "dog", "dog"]
    #          y_pred = [0, 1, 0, 1, 1]

    # truelabel_matrix {
    #   0: ["cat", "cat"],
    #   1: ["dog", "dog", "dog"]
    # }

    truelabel_matrix = {}
    for true, pred in zip(y_true, y_pred): 
        truelabel_matrix.setdefault(pred, []).append(true)

    total = 0
    for cluster in truelabel_matrix.values(): # count true labels for every 
                                              # cluster
        # in the example above we have cluster 0: "cat" most common with count 2 
        # cluster 1: "dog" most common with 3
        # total = 5
        most_common_label = Counter(cluster).most_common(1)[0][1]
        total += most_common_label
    return total / len(y_true)

# clustering
def cluster_and_evaluate(features, true_labels, n_clusters):
    print(f"\nRunning KMeans with {n_clusters} clusters...")

    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    pred_labels = kmeans.fit_predict(features)

    purity = purity_score(true_labels, pred_labels)

    print(f"\nClustering evaluation:")
    print(f"Purity: {purity:.4f}")

    return pred_labels, purity

n_classes = len(np.unique(test_labels))  # 100 for CIFAR-100
print("number of dataset classes: ", n_classes)
#scaled_features = scaled_softmax(test_features)
pred_labels, purity = cluster_and_evaluate(test_features, test_labels, n_clusters=n_classes)
